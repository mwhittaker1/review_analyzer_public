{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1df936eb",
   "metadata": {},
   "source": [
    "Using OpenAPI 4.0-Turbo perform sentiment analysis on return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0714f923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import duckdb\n",
    "from typing import Optional, Tuple\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32788b41",
   "metadata": {},
   "source": [
    "## Define key variables\n",
    "- file_path - import file path, xlsx, csv, or parquet\n",
    "- tname - Duckdb table name to import to\n",
    "- db_path - duckdb location, creates if not present\n",
    "- row_id - unique identifier row to create (assumes no unique ID)\n",
    "- comment_column - The name of the column containing the comment to be analyzed\n",
    "- con -establishes duckdb.connect to db_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fad8b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define important parameters\n",
    "file_path = r'C:\\Code\\URBN\\review_analyzer\\data\\IID_RR_GROSS.csv'\n",
    "tname = 'iid_return_reasonsv3'\n",
    "db_path='iid_return_commentv3'\n",
    "row_id = \"row_id\"\n",
    "# Define the column with returns comments\n",
    "comment_column = \"RETURN_COMMENT\"\n",
    "con = duckdb.connect(db_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495a6e0d",
   "metadata": {},
   "source": [
    "CSV File Processing\n",
    "\n",
    "Sets SQL mode based on clear parameter (OR REPLACE TABLE or TABLE IF NOT EXISTS)\n",
    "Implements a three-attempt strategy with progressively more lenient options:\n",
    "First Attempt:\n",
    "\n",
    "Uses standard options with CSV auto-detection\n",
    "Adds a row_id column using ROW_NUMBER() window function\n",
    "Second Attempt (if first fails):\n",
    "\n",
    "Adds strict_mode=false to handle unterminated quotes\n",
    "Prints failure message from first attempt\n",
    "Third Attempt (not shown in the visible snippet but likely continues):\n",
    "\n",
    "Would likely add more permissive options if second attempt fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bccfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(\n",
    "        fname: str,\n",
    "        clear=True,\n",
    "        db_path='db_path',\n",
    "        tname='staging_table',\n",
    "        ftype=None,\n",
    "        ):\n",
    "    \"\"\"\n",
    "    load CSV, Parquet, or Excel file into a DuckDB table.\n",
    "    Returns confirmation message.\n",
    "    clear = Replace existing table if true, else create if not exists.\n",
    "    Adds a unique row_id column to each record.\n",
    "\n",
    "    Returnss: print statement confirmation of import.\n",
    "    \"\"\"\n",
    "    import os\n",
    "\n",
    "\n",
    "    # Infer file type if not provided\n",
    "    if ftype is None:\n",
    "        ext = os.path.splitext(fname)[1].lower()\n",
    "        if ext == '.csv':\n",
    "            ftype = 'csv'\n",
    "        elif ext == '.parquet':\n",
    "            ftype = 'parquet'\n",
    "        elif ext in ('.xlsx', '.xls'):\n",
    "            ftype = 'excel'\n",
    "        else:\n",
    "            con.close()\n",
    "            raise ValueError(\"Unsupported file extension.\")\n",
    "\n",
    "    if ftype == 'csv':\n",
    "        mode = 'OR REPLACE TABLE' if clear else 'TABLE IF NOT EXISTS'\n",
    "        try:\n",
    "            # First attempt with standard options, add row_id\n",
    "            con.execute(f\"\"\"\n",
    "                CREATE {mode} {tname} AS\n",
    "                SELECT *, ROW_NUMBER() OVER () AS row_id FROM read_csv_auto('{fname}', escape='\\\\', encoding='utf-8', header=True)\n",
    "            \"\"\")\n",
    "        except Exception as e:\n",
    "            print(f\"First attempt failed, trying with strict_mode=false: {str(e)}\")\n",
    "            try:\n",
    "                # Second attempt with strict_mode=false to handle unterminated quotes\n",
    "                con.execute(f\"\"\"\n",
    "                    CREATE {mode} {tname} AS\n",
    "                    SELECT *, ROW_NUMBER() OVER () AS row_id FROM read_csv_auto(\n",
    "                        '{fname}', \n",
    "                        escape='\\\\',\n",
    "                        encoding='utf-8',\n",
    "                        header=True,\n",
    "                        strict_mode=false\n",
    "                    )\n",
    "                \"\"\")\n",
    "            except Exception as e2:\n",
    "                print(f\"Second attempt failed, trying with ignore_errors=true: {str(e2)}\")\n",
    "                try:\n",
    "                    # Third attempt with ignore_errors=true to skip problematic rows\n",
    "                    con.execute(f\"\"\"\n",
    "                        CREATE {mode} {tname} AS\n",
    "                        SELECT *, ROW_NUMBER() OVER () AS row_id FROM read_csv_auto(\n",
    "                            '{fname}', \n",
    "                            escape='\\\\',\n",
    "                            encoding='utf-8',\n",
    "                            header=True,\n",
    "                            strict_mode=false,\n",
    "                            ignore_errors=true\n",
    "                        )\n",
    "                    \"\"\")\n",
    "                except Exception as e3:\n",
    "                    con.close()\n",
    "                    raise ValueError(f\"Failed to import CSV after multiple attempts: {str(e3)}\")\n",
    "    elif ftype == 'parquet':\n",
    "        mode = 'OR REPLACE TABLE' if clear else 'TABLE IF NOT EXISTS'\n",
    "        con.execute(f\"\"\"\n",
    "            CREATE {mode} {tname} AS\n",
    "            SELECT *, ROW_NUMBER() OVER () AS row_id FROM read_parquet('{fname}')\n",
    "        \"\"\")\n",
    "    elif ftype == 'excel':\n",
    "        df = pd.read_excel(fname)\n",
    "        if clear:\n",
    "            con.execute(f\"DROP TABLE IF EXISTS {tname}\")\n",
    "        # Add row_id column to DataFrame\n",
    "        df['row_id'] = range(1, len(df) + 1)\n",
    "        con.register('temp_excel_df', df)\n",
    "        con.execute(f\"CREATE TABLE {tname} AS SELECT * FROM temp_excel_df\")\n",
    "        con.unregister('temp_excel_df')\n",
    "    else:\n",
    "        con.close()\n",
    "        raise ValueError(\"Unsupported file type.\")\n",
    "\n",
    "    return print(f\"Import completed: {fname} into {tname} at {db_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3568c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run import data, skip function if data already exists in duckdb.\n",
    "import_data(file_path, db_path=db_path,tname=tname, clear=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fe9652",
   "metadata": {},
   "source": [
    "Pull data from duckdb into DataFrame\n",
    "- is_sample fetches 500 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca18010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_return_comments(con, tname, is_sample=False, comment_column='RETURN COMMENT', row_id='row_id'):\n",
    "    \"\"\"\n",
    "    Extract return comments from the DuckDB table with a row_id for later matching.\n",
    "    con: DuckDB connection\n",
    "    tname: Table name\n",
    "    is_sample: Whether to take a sample or full dataset\n",
    "    comment_column: Column containing return comments\n",
    "    row_id: Column to use for joining results back to main dataset\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with comments and row_id\n",
    "    \"\"\"\n",
    "    # Filter for non-empty comments\n",
    "    comment_filter = f\"\"\"WHERE \"{comment_column}\" IS NOT NULL AND TRIM(\"{comment_column}\") != ''\"\"\"\n",
    "    \n",
    "    if is_sample:\n",
    "        sample_query = \"ORDER BY RANDOM() LIMIT 500\"  \n",
    "    else:\n",
    "        sample_query = \"\"\n",
    "    \n",
    "    # Select the comment, row_id, and add row identifier\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        \"{row_id}\" as row_id,\n",
    "        \"{comment_column}\" as comment,\n",
    "    FROM {tname}\n",
    "    {comment_filter}\n",
    "    {sample_query}\n",
    "    \"\"\"\n",
    "    \n",
    "    result = con.execute(query).df()\n",
    "    print(f\"Extracted {len(result)} comments from {tname}\")\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ee3776",
   "metadata": {},
   "source": [
    "Set Client to API key used for OpenAi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eacbf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apikey import create_secret\n",
    "\n",
    "client = create_secret()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151f7b55",
   "metadata": {},
   "source": [
    "Sends Client, Prompt, and DataFrame to OpenAI for actioning.\n",
    "- client: API key\n",
    "- prompt: text prompt for OpenAI\n",
    "- df: DataFrame of data\n",
    "- debug: verbose logging for send/receive\n",
    "- gpt_model: ChatGPT model to use.\n",
    "- sys_prompt: System message to send prior to user messages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d141bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import openai\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "def ai_analyze_comments(client,sys_prompt: str, prompt: str, df: pd.DataFrame, debug: bool = True, gpt_model=\"gpt-4o\") -> str:\n",
    "    \"\"\"\n",
    "    Sends `prompt` plus the JSON version of `df` to ChatGPT,\n",
    "    and returns the model's response.strip()\n",
    "    \"\"\"\n",
    "\n",
    "    df_json = df.to_json(orient=\"records\")\n",
    "    if debug:\n",
    "        print(\"Prompt sent to model:\\n\", prompt)\n",
    "\n",
    "    messages = [\n",
    "    {\"role\": \"system\",\n",
    "    \"content\": \n",
    "        \"You are an expert linguistic analyst specializing in extracting and scoring themes from customer return comments. \"\n",
    "        \"You always return your output exactly in the structure specified in the user's instructions. \"\n",
    "        \"Be precise, consistent, and strictly follow the output schema and scoring rules provided.\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": df_json}\n",
    "    ]\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages,\n",
    "        temperature=0.1,\n",
    "        max_tokens=15000,\n",
    "    )\n",
    "\n",
    "    content = resp.choices[0].message.content.strip()\n",
    "\n",
    "    if debug:\n",
    "        print(\"Raw response from OpenAI:\\n\", content)\n",
    "    if not content:\n",
    "        raise ValueError(\"Empty response from OpenAI\")\n",
    "\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf4b3f8",
   "metadata": {},
   "source": [
    "Fucntions to clean data of characters that can impact ChatGPT's ability to parse correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4806d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def prepare_data_for_analysis(text):\n",
    "    \"\"\"Sanitize comment text and strip code block formatting from model output.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Remove control characters and excessive whitespace\n",
    "    text = re.sub(r'[\\x00-\\x1F\\x7F]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def strip_code_block(text):\n",
    "    \"\"\"Remove Markdown code block wrappers from OpenAI response.\"\"\"\n",
    "    text = text.strip()\n",
    "    code_block_pattern = r\"^```(?:json)?\\s*([\\s\\S]*?)\\s*```$\"\n",
    "    match = re.match(code_block_pattern, text)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405617a8",
   "metadata": {},
   "source": [
    "Fetch comments_df - which is the row_id and comment only, after analysis this will be rejoined based on the row_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c3a916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE - Fetch a sample of comments and row_id only (for analysis and joining)\n",
    "#comments_df = fetch_return_comments(con, tname, is_sample=False, comment_column=comment_column, row_id=row_id)\n",
    "\n",
    "# Full set\n",
    "comments_df = fetch_return_comments(con, tname, is_sample=False, comment_column=comment_column, row_id=row_id)\n",
    "\n",
    "# Show counts\n",
    "print(f\"Comments sample rows: {len(comments_df)}\")\n",
    "\n",
    "# Display the first few rows of comments\n",
    "if len(comments_df) > 0:\n",
    "    print(\"\\nSample of comments:\")\n",
    "    for i, comment in enumerate(comments_df['comment'].head(3)):\n",
    "        print(f\"{i+1}: {comment[:100]}...\")\n",
    "else:\n",
    "    print(\"WARNING: No valid comments found in the sample\")\n",
    "\n",
    "comments_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c9bf69",
   "metadata": {},
   "source": [
    "Hardcode batching if needed for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26e3fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch1 = comments_df.iloc[:20]\n",
    "print(f\"confirming batch size for batch 1 : {len(batch1)}\")\n",
    "batch2 = comments_df.iloc[21:75]\n",
    "print(f\"confirming batch size for batch 1 : {len(batch2)}\")\n",
    "batch3 = comments_df.iloc[76:276]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cebfec3",
   "metadata": {},
   "source": [
    "Function to run pipeline on extracted data\n",
    "- Prepares DF with striping of special characters\n",
    "- Sets and manages batching as per batch_size\n",
    "- Runs Product Analysis using product_prompt, then again with customer_sentiment_prompt\n",
    "- For each batch, \n",
    "- - strips special characters in DataFrame\n",
    "- - unnests \"theme\" .json which contains 1-4 themes as per current scripts\n",
    "- - catches failed batches, and attempts 1 re-run after full processing\n",
    "- - saves each batch as a .csv {type}_batch_x.csv\n",
    "- combines all batches into a combined .csv\n",
    "- Runs the above for Customer_sentiment\n",
    "- combines all customer_sentiments into a single .csv\n",
    "- returns two DataFrames, one for each analysis type as a row_id, {data}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fedb24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_sentiment_analysis(\n",
    "    comments_df,\n",
    "    client=None,\n",
    "    product_prompt: str = 'prompts/product_prompt.txt',\n",
    "    customer_prompt: str = 'prompts/customer_sentiment_prompt.txt',\n",
    "    batch_size: int = 500,\n",
    "    debug: bool = False\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Run both Product and Customer analysis on same data set, batching results and saving each batch to CSV.\n",
    "    At the end, create a combined CSV of all results.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    from io import StringIO\n",
    "    import traceback\n",
    "\n",
    "    print(f\"Starting analysis at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Total records to process: {len(comments_df)}\")\n",
    "\n",
    "    # Helper to batch DataFrame\n",
    "    def batch_df(df, batch_size):\n",
    "        for i in range(0, len(df), batch_size):\n",
    "            yield i // batch_size + 1, df.iloc[i:i+batch_size]\n",
    "\n",
    "    # Clean and sanitize comments before batching\n",
    "    comments_df = comments_df.copy()\n",
    "    if 'comment' in comments_df.columns:\n",
    "        comments_df['comment'] = comments_df['comment'].apply(prepare_data_for_analysis)\n",
    "\n",
    "    # PRODUCT FEEDBACK ANALYSIS\n",
    "    print(\"\\n===== PRODUCT FEEDBACK ANALYSIS =====\")\n",
    "    print(f\"using product prompt: {product_prompt}\")\n",
    "    product_batches = []\n",
    "    for batch_num, batch_df_ in batch_df(comments_df, batch_size):\n",
    "        print(f\"Processing product batch {batch_num} with {len(batch_df_)} records...\")\n",
    "        print(f\"Batch {batch_num} - DataFrame sent to OpenAI:\")\n",
    "        print(batch_df_)\n",
    "        product_result = ai_analyze_comments(\n",
    "            client=client,\n",
    "            prompt=product_prompt,\n",
    "            df=batch_df_,\n",
    "            debug=debug\n",
    "        )\n",
    "        # Logging before parsing\n",
    "        print(f\"Batch {batch_num} - Raw OpenAI response type: {type(product_result)}\")\n",
    "        print(f\"Batch {batch_num} - Raw OpenAI response length: {len(product_result) if isinstance(product_result, str) else 'N/A'}\")\n",
    "        print(f\"Batch {batch_num} - Raw OpenAI response:\\n{product_result}\")\n",
    "        try:\n",
    "            cleaned_result = strip_code_block(product_result)\n",
    "            import json\n",
    "            parsed_json = json.loads(cleaned_result)\n",
    "            \n",
    "            # If 'themes' is a nested list of dicts, create columns for each theme\n",
    "            if isinstance(parsed_json, list) and 'themes' in parsed_json[0]:\n",
    "                # Create a base dataframe with non-theme data\n",
    "                base_records = []\n",
    "                for record in parsed_json:\n",
    "                    # Extract the base record without themes\n",
    "                    base_record = {k: v for k, v in record.items() if k != 'themes'}\n",
    "                    \n",
    "                    # Add theme data as separate columns\n",
    "                    themes = record.get('themes', [])\n",
    "                    for i, theme_data in enumerate(themes, 1):\n",
    "                        base_record[f'Theme_{i}'] = theme_data.get('theme', '')\n",
    "                        base_record[f'Sentiment_{i}'] = theme_data.get('sentiment', 0)\n",
    "                    \n",
    "                    base_records.append(base_record)\n",
    "                \n",
    "                product_result_df = pd.DataFrame(base_records)\n",
    "            else:\n",
    "                product_result_df = pd.DataFrame(parsed_json)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSONDecodeError for product batch {batch_num}: {e}\")\n",
    "            with open(f\"failed_product_batch_{batch_num}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(product_result)\n",
    "            product_result_df = pd.DataFrame()\n",
    "        except Exception as e:\n",
    "            print(f\"Exception for product batch {batch_num}: {e}\")\n",
    "            with open(f\"failed_product_batch_{batch_num}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(product_result)\n",
    "            product_result_df = pd.DataFrame()\n",
    "        product_batches.append(product_result_df)\n",
    "        batch_csv = f\"product_batch{batch_num}.csv\"\n",
    "        product_result_df.to_csv(batch_csv, index=False)\n",
    "        print(f\"Saved product batch {batch_num} to {batch_csv}\")\n",
    "\n",
    "    # Combine all product batches\n",
    "    product_df = pd.concat(product_batches, ignore_index=True) if product_batches else pd.DataFrame()\n",
    "    product_df.to_csv(\"product_all_batches_combined.csv\", index=False)\n",
    "    print(\"Saved combined product results to product_all_batches_combined.csv\")\n",
    "\n",
    "    # CUSTOMER FEEDBACK ANALYSIS\n",
    "    print(\"\\n===== CUSTOMER FEEDBACK ANALYSIS =====\")\n",
    "    print(f\"using customer prompt: {customer_prompt}\")\n",
    "    customer_batches = []\n",
    "    for batch_num, batch_df_ in batch_df(comments_df, batch_size):\n",
    "        print(f\"Processing customer batch {batch_num} with {len(batch_df_)} records...\")\n",
    "        print(f\"Batch {batch_num} - DataFrame sent to OpenAI:\")\n",
    "        print(batch_df_)\n",
    "        customer_result = ai_analyze_comments(\n",
    "            client=client,\n",
    "            prompt=customer_prompt,\n",
    "            df=batch_df_,\n",
    "            debug=debug\n",
    "        )\n",
    "        # Logging before parsing\n",
    "        print(f\"Batch {batch_num} - Raw OpenAI response type: {type(customer_result)}\")\n",
    "        print(f\"Batch {batch_num} - Raw OpenAI response length: {len(customer_result) if isinstance(customer_result, str) else 'N/A'}\")\n",
    "        print(f\"Batch {batch_num} - Raw OpenAI response:\\n{customer_result}\")\n",
    "        try:\n",
    "            cleaned_result = strip_code_block(customer_result)\n",
    "            import json\n",
    "            parsed_json = json.loads(cleaned_result)\n",
    "            \n",
    "            # If 'themes' is a nested list of dicts, create columns for each theme\n",
    "            if isinstance(parsed_json, list) and 'themes' in parsed_json[0]:\n",
    "                # Create a base dataframe with non-theme data\n",
    "                base_records = []\n",
    "                for record in parsed_json:\n",
    "                    # Extract the base record without themes\n",
    "                    base_record = {k: v for k, v in record.items() if k != 'themes'}\n",
    "                    \n",
    "                    # Add theme data as separate columns\n",
    "                    themes = record.get('themes', [])\n",
    "                    for i, theme_data in enumerate(themes, 1):\n",
    "                        base_record[f'Theme_{i}'] = theme_data.get('theme', '')\n",
    "                        base_record[f'Sentiment_{i}'] = theme_data.get('sentiment', 0)\n",
    "                    \n",
    "                    base_records.append(base_record)\n",
    "                \n",
    "                customer_result_df = pd.DataFrame(base_records)\n",
    "            else:\n",
    "                customer_result_df = pd.DataFrame(parsed_json)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSONDecodeError for customer batch {batch_num}: {e}\")\n",
    "            with open(f\"failed_customer_batch_{batch_num}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(customer_result)\n",
    "            customer_result_df = pd.DataFrame()\n",
    "        except Exception as e:\n",
    "            print(f\"Exception for customer batch {batch_num}: {e}\")\n",
    "            with open(f\"failed_customer_batch_{batch_num}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(customer_result)\n",
    "            customer_result_df = pd.DataFrame()\n",
    "        customer_batches.append(customer_result_df)\n",
    "        batch_csv = f\"customer_batch{batch_num}.csv\"\n",
    "        customer_result_df.to_csv(batch_csv, index=False)\n",
    "        print(f\"Saved customer batch {batch_num} to {batch_csv}\")\n",
    "\n",
    "    # Combine all customer batches\n",
    "    customer_df = pd.concat(customer_batches, ignore_index=True) if customer_batches else pd.DataFrame()\n",
    "    customer_df.to_csv(\"customer_all_batches_combined.csv\", index=False)\n",
    "    print(\"Saved combined customer results to customer_all_batches_combined.csv\")\n",
    "\n",
    "    return product_df, customer_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684f2923",
   "metadata": {},
   "source": [
    "Run both Customer and Product Analysis\n",
    "\n",
    "CHECK BATCH #!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6f78b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "cust_prompt = Path(r\"prompts\\customer_sentiment_prompt.txt\").read_text(encoding=\"utf8\")\n",
    "prod_prompt = Path(r\"prompts\\product_prompt.txt\").read_text(encoding=\"utf8\")\n",
    "sys_prompt = Path(r\"prompts\\system_prompt.txt\").read_text(encoding=\"utf8\")\n",
    "\n",
    "analyzed_product, analyzed_customer = handle_sentiment_analysis(\n",
    "    batch2, # Make sure correct batch is passed!\n",
    "    client=client,\n",
    "    sys_prompt=sys_prompt,\n",
    "    batch_size= 25,\n",
    "    debug= False,\n",
    "    id_column= 'row_id',\n",
    "    db_path= db_path,\n",
    "    product_prompt=prod_prompt,\n",
    "    customer_prompt=cust_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a8d495",
   "metadata": {},
   "source": [
    "Creates a new duckdb table to store analyzed data\n",
    "\n",
    "Expects customer and Product data to be combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc6d20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_completed_analytics_table(con):\n",
    "\n",
    "    con.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS completed_analytics (\n",
    "    row_id INTEGER,\n",
    "    -- Product analytics\n",
    "    RETURN_COMMENT_p VARCHAR,\n",
    "    \"Theme 1_p\" VARCHAR,\n",
    "    \"Sentiment 1_p\" DOUBLE,\n",
    "    \"Theme 2_p\" VARCHAR,\n",
    "    \"Sentiment 2_p\" DOUBLE,\n",
    "    \"Theme 3_p\" VARCHAR,\n",
    "    \"Sentiment 3_p\" DOUBLE,\n",
    "    \"Theme 4_p\" VARCHAR,\n",
    "    \"Sentiment 4_p\" DOUBLE,\n",
    "    \"Pos_mean_p\" DOUBLE,\n",
    "    \"Neg_mean_p\" DOUBLE,\n",
    "    \"Total_sentiment_p\" DOUBLE,\n",
    "    -- Customer analytics\n",
    "    RETURN_COMMENT_c VARCHAR,\n",
    "    \"Theme 1_c\" VARCHAR,\n",
    "    \"Sentiment 1_c\" DOUBLE,\n",
    "    \"Theme 2_c\" VARCHAR,\n",
    "    \"Sentiment 2_c\" DOUBLE,\n",
    "    \"Theme 3_c\" VARCHAR,\n",
    "    \"Sentiment 3_c\" DOUBLE,\n",
    "    \"Theme 4_c\" VARCHAR,\n",
    "    \"Sentiment 4_c\" DOUBLE,\n",
    "    \"Pos_mean_c\" DOUBLE,\n",
    "    \"Neg_mean_c\" DOUBLE,\n",
    "    \"Total_sentiment_c\" DOUBLE,\n",
    "    processed_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e215383a",
   "metadata": {},
   "source": [
    "Combines product_df and customer_df into new table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2262a9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sentiment_only_to_duckdb(con, analyzed_product_df, analyzed_customer_df, tname):\n",
    "    \n",
    "    # Rename columns for product and customer\n",
    "    def rename_cols(df, suffix):\n",
    "        rename_map = {}\n",
    "        for col in df.columns:\n",
    "            if col != 'row_id':  # Keep row_id as is for joining\n",
    "                rename_map[col] = f\"{col}_{suffix}\"\n",
    "        return df.rename(columns=rename_map)\n",
    "    \n",
    "    product_df = rename_cols(analyzed_product_df, 'p')\n",
    "    customer_df = rename_cols(analyzed_customer_df, 'c')\n",
    "\n",
    "    # Merge product and customer on row_id\n",
    "    combined_df = pd.merge(product_df, customer_df, on='row_id', how='outer')\n",
    "\n",
    "    # Ensure table is created before inserting data\n",
    "    create_completed_analytics_table(con)\n",
    "\n",
    "    con.register('batch_results', combined_df)\n",
    "    con.execute(\"\"\"\n",
    "    INSERT INTO completed_analytics (\n",
    "        row_id,\n",
    "        -- Product analytics\n",
    "        RETURN_COMMENT_p, \"Theme 1_p\", \"Sentiment 1_p\", \"Theme 2_p\", \"Sentiment 2_p\", \n",
    "        \"Theme 3_p\", \"Sentiment 3_p\", \"Theme 4_p\", \"Sentiment 4_p\", \n",
    "        \"Pos_mean_p\", \"Neg_mean_p\", \"Total_sentiment_p\",\n",
    "        -- Customer analytics\n",
    "        RETURN_COMMENT_c, \"Theme 1_c\", \"Sentiment 1_c\", \"Theme 2_c\", \"Sentiment 2_c\", \n",
    "        \"Theme 3_c\", \"Sentiment 3_c\", \"Theme 4_c\", \"Sentiment 4_c\", \n",
    "        \"Pos_mean_c\", \"Neg_mean_c\", \"Total_sentiment_c\"\n",
    "    )\n",
    "    SELECT \n",
    "        br.row_id,\n",
    "        -- Product analytics\n",
    "        br.RETURN_COMMENT_p, br.\"Theme 1_p\", br.\"Sentiment 1_p\", br.\"Theme 2_p\", br.\"Sentiment 2_p\", \n",
    "        br.\"Theme 3_p\", br.\"Sentiment 3_p\", br.\"Theme 4_p\", br.\"Sentiment 4_p\", \n",
    "        br.\"Pos_mean_p\", br.\"Neg_mean_p\", br.\"Total_sentiment_p\",\n",
    "        -- Customer analytics\n",
    "        br.RETURN_COMMENT_c, br.\"Theme 1_c\", br.\"Sentiment 1_c\", br.\"Theme 2_c\", br.\"Sentiment 2_c\", \n",
    "        br.\"Theme 3_c\", br.\"Sentiment 3_c\", br.\"Theme 4_c\", br.\"Sentiment 4_c\", \n",
    "        br.\"Pos_mean_c\", br.\"Neg_mean_c\", br.\"Total_sentiment_c\"\n",
    "    FROM batch_results br\n",
    "    WHERE NOT EXISTS (\n",
    "        SELECT 1 FROM completed_analytics ca \n",
    "        WHERE ca.row_id = br.row_id\n",
    "    )\n",
    "    \"\"\")\n",
    "\n",
    "    # Confirm the insertion\n",
    "    count = con.execute(\"SELECT COUNT(*) FROM completed_analytics\").fetchone()[0]\n",
    "    print(f\"Total records in completed_analytics: {count}, current batch started with: {len(combined_df)}\")\n",
    "\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b4b649",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_customer_prod_df = save_sentiment_only_to_duckdb(con, analyzed_product, analyzed_customer, tname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fb0487",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
